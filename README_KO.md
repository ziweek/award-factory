# award-factory

<img src="./src/banner_notion.png"/>

<p align="center">
  <br/>
  <strong>ğŸ“ êµ¬ê¸€ ë¨¸ì‹ ëŸ¬ë‹ ë¶€íŠ¸ìº í”„ ì¶œí’ˆ ì‘í’ˆ ğŸ“</strong>
  <br/>
  <br/>
  <a href='https://github.com/ziweek/award-factory/blob/main/README_KO.md'>
    KOREAN
  </a>
  &nbsp;|&nbsp;
  <a href='https://github.com/ziweek/award-factory/blob/main/README.md'>
    ENGLISH
  </a>
  <br/>
  <br/>
  <strong>ìƒì¥ ê³µì¥: ìœ ì¾Œí•˜ê²Œ ì¬ëŠ¥ ìˆëŠ” ìƒì„±í˜• AIê°€ ë‹¹ì‹ ì„ ìœ„í•´ ì •ì„±ìŠ¤ëŸ½ê²Œ ë§Œë“  ìƒì¥</strong>
  <br/>
  <br/>
  <a href='https://paperswithcode.com/paper/gemma-open-models-based-on-gemini-research'>
    <img src="https://img.shields.io/badge/Paperswithcode-Gemma:%20Open%20Models%20Based on%20Gemini%20Research%20and%20Technology-25c2a0?style=flat-square"/>
  </a>
  <br/>
  <img src="https://img.shields.io/badge/Next.js-000000?style=flat-square&logo=nextdotjs&logoColor=white"/>
  <img src="https://img.shields.io/badge/PWA-5A0FC8?style=flat-square&logo=pwa&logoColor=white"/> 
  <img src="https://img.shields.io/badge/FastAPI-009688?style=flat-square&logo=fastapi&logoColor=white"/>
  <br/>
  <img src="https://img.shields.io/badge/Docker-2496ED?style=flat-square&logo=Docker&logoColor=white"/>
  <img src="https://img.shields.io/badge/Jenkins-D24939?style=flat-square&logo=jenkins&logoColor=white"/>
  <img src="https://img.shields.io/badge/AWS-232F3E?style=flat-square&logo=amazonwebservices&logoColor=white"/>
</p>
<br/>

<p align="center">  
  <strong>ì•„ë˜ì˜ ë±ƒì§€ì—ì„œ í”„ë¡œí† íƒ€ì…ì„ í™•ì¸í•´ë³´ì„¸ìš”.<strong>
  <br/>
  <br/>
  <a href='https://award-factory.vercel.app'>
    <img src="https://img.shields.io/badge/Website-Vercel-000000?style=flat-square&logo=vercel&logoColor=white"/>
  </a>
  <a href='https://huggingface.co/ziweek/gemma-2b-it-award-factory'>
      <img src="https://img.shields.io/badge/Model-Hugging%20Face-FFD21E?style=flat-square&logo=huggingface&logoColor=white"/>
  </a>
  <a href='https://huggingface.co/datasets/ziweek/award-factory-citation'>
      <img src="https://img.shields.io/badge/Dataset-Hugging%20Face-FFD21E?style=flat-square&logo=huggingface&logoColor=white"/>
  </a>
</p>

<br/>
<br/>

# 1. ê°œìš”

> [!NOTE]
>
> - ì´ í”„ë¡œì íŠ¸ëŠ” ëˆ„êµ¬ë‚˜ ê°„í¸í•˜ê²Œ ëª‡ ë¶„ ì•ˆì— ë§ì¶¤í˜• ìƒì¥ì„ ì œì‘í•˜ì—¬, íƒ€ì¸ì„ ì†ì‰½ê²Œ ì¶•í•˜í•˜ê³  ê²©ë ¤í•  ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
> - ìƒì¥ ê³µì¥ì€ í–‰ë³µì„ ë‚˜ëˆ„ê¸° ìœ„í•´ ê¸°íšëœ í”„ë¡œì íŠ¸ë¡œ, ë¶€ëª¨ë‹˜ê»˜ íŠ¹ë³„í•œ ìƒì¥ì„ ë“œë¦¬ìëŠ” ì•„ì´ë””ì–´ì—ì„œ ì¶œë°œí–ˆìŠµë‹ˆë‹¤. ì§€ì† ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•´ í”„ë¡ íŠ¸ì—”ë“œ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, Google Gemma:2b ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì—¬ ì‚¬ìš©ì ë§ì¶¤í˜• ìƒì¥ ë¬¸êµ¬ë¥¼ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ëŠ” ì„œë²„ ìš´ì˜ ë¹„ìš© ë¬¸ì œë¡œ ì„œë¹„ìŠ¤ê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ì§€ë§Œ, ë°ëª¨ëŠ” Huggingfaceì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> - QLoRA ì–‘ìí™” ë° llama-cpp ìµœì í™”ì™€ ê°™ì€ ìµœì‹  ê¸°ìˆ ì„ ì ìš©í•´ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì´ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼œ, í–¥í›„ ë” íš¨ìœ¨ì ì¸ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.

https://github.com/user-attachments/assets/2def17e0-46ea-4561-8b50-fc78d595b88b

<br/>
<br/>

# Implementation

<table>
  <tr>
    <td style="width:1/2;">
      <img src="./src/diagram.png"/>
    </td>
  </tr>
</table>

<details open>
 <summary><b>Google Gemma:2B Finetuning</b></summary>
Implemented prompt engineering and QLoRA-based quantization fine-tuning using the Google/Gemma-2b-it model with PEFT techniques to optimize personalized award text generation tailored to user preferences.
</details>
<br/>

<details open>
 <summary><b>llama-cpp Quantization</b></summary>
Applied quantization with the Q5_K_M option in llama-cpp, achieving a 63.3% reduction in model size and an 83.4% decrease in inference time without compromising performance, enabling faster and more efficient service.

<br/>

```
$ llama.cpp/llama-quantize gguf_model/gemma-2b-it-award-factory-v2.gguf gguf_model/gemma-2b-it-award-factory-v2.gguf-Q5_K_M.gguf Q5_K_M

...
llama_model_quantize_internal: model size  =  4780.29 MB
llama_model_quantize_internal: quant size  =  1748.67 MB

main: quantize time = 17999.81 ms
main:    total time = 17999.81 ms
```

```
$ ollama list

NAME                    ID              SIZE      MODIFIED
award-factory:q5        8df06172b64b    1.8 GB    19 seconds ago
award-factory:latest    ae186115cc83    5.0 GB    28 minutes ago
```

</details>
<br/>

<details open>
  <summary><b>Docker-compose</b></summary>
Utilized Docker Compose to containerize the backend and frontend services, ensuring consistency in deployment environments and facilitating scalable and maintainable full-stack web application development.
</details>
<br/>

# Contribution

<a href="https://github.com/ziweek/award-factory/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=ziweek/award-factory" />
</a>
